\documentclass[12pt, letterpaper, titlepage]{article}

\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref, url}
\hypersetup{colorlinks = true, linkcolor = blue, citecolor=blue, urlcolor =
  blue}
\usepackage{natbib}
\usepackage{enumitem}
\usepackage{setspace}

\usepackage[pagewise]{lineno}
%\linenumbers*[1]
% %% patches to make lineno work better with amsmath
\newcommand*\patchAmsMathEnvironmentForLineno[1]{%
 \expandafter\let\csname old#1\expandafter\endcsname\csname #1\endcsname
 \expandafter\let\csname oldend#1\expandafter\endcsname\csname end#1\endcsname
 \renewenvironment{#1}%
 {\linenomath\csname old#1\endcsname}%
 {\csname oldend#1\endcsname\endlinenomath}}%
\newcommand*\patchBothAmsMathEnvironmentsForLineno[1]{%
 \patchAmsMathEnvironmentForLineno{#1}%
 \patchAmsMathEnvironmentForLineno{#1*}}%

\AtBeginDocument{%
 \patchBothAmsMathEnvironmentsForLineno{equation}%
 \patchBothAmsMathEnvironmentsForLineno{align}%
 \patchBothAmsMathEnvironmentsForLineno{flalign}%
 \patchBothAmsMathEnvironmentsForLineno{alignat}%
 \patchBothAmsMathEnvironmentsForLineno{gather}%
 \patchBothAmsMathEnvironmentsForLineno{multline}%
}

% control floats
\renewcommand\floatpagefraction{.9}
\renewcommand\topfraction{.9}
\renewcommand\bottomfraction{.9}
\renewcommand\textfraction{.1}
\setcounter{totalnumber}{50}
\setcounter{topnumber}{50}
\setcounter{bottomnumber}{50}

\newcommand{\jy}[1]{\textcolor{blue}{JY: #1}}
\newcommand{\eds}[1]{\textcolor{red}{EDS: (#1)}}


\title{How Large a Sample Size is Needed for Block Bootstrap Confidence Intrvals
  to Have Desired Coverage Rates?}

\author{Mathew Chandy
%   \href{mailto:mathew.chandy@uconn.edu}
% {\nolinkurl{mathew.chandy@uconn.edu}}\\
  \and Jun Yan\\[1ex]
  Department of Statistics, University of Connecticut\\
}
\date{}

\begin{document} 
\maketitle

\begin{abstract}
Block bootstrap is a method that is useful for estimating a parameter of a time
series. Theoretically, the method will work perfectly given an infinitely
large sample of the time series. It is necessary to know how large of a finite
sample is required for block bootstrap estimation to work. Before this study,
there have been no investigations into this problem within the literature. The
aim of this study is to answer this question by simulating replications of
block bootstrap interval estimations of a parameter of a time series, and
recording the rate at which the parameter is recovered by each replicate
interval. The sample length required for acceptable performance is
unsurprisingly found to be dependent on the type of interval construction and
the series' level of temporal dependence.


\bigskip
\noindent{\sc Keywords}:
dependent data; gsimulation
\end{abstract}

\doublespace

\jy{Practice my writing tips on latex (e.g., keep line width under 80; use
  double blank lines to separate paragraphs; etc.):
  \url{
https://statds.github.io/stat-writing/using-the-right-tools-for-writing.html},
  chapter 2}

\jy{turn on the column number mode of your editor. Keep the width to under 80.}


\section{Introduction}
\label{sec:intro}

Block bootstrap is a powerful tool for constructing confidence intervals when
making inferences about dependent data. Early ideas were developed
independently by \citet{hall1985resampling}, \citet{carlstein1986use}, and 
\citet{kunsch1989jackknife}.
It has since been applied in various fields such as econometrics
\citep{mackinnon2006bootstrap} and meteorology \citep{varga2017generalised}.
Block bootstrap is particularly useful for serially dependent
data when the serial dependence is not specified or not of primary interest.
The method is expected to produce confidence intervals with coverage rates
matching their nominal levels as the sample size grows to infinity. However,
when dealing with finite sample sizes, an important question is how large the
sample size needs to be for block bootstrap confidence intervals to have the
desired coverage rates.


The necessary sample size for bootstrap standard errors to provide valid
uncertainty measures in practice has not been extensively studied. For
independent data and non-block bootstrap, \citet{hesterberg2015teachers} notes
that while percentile-based confidence intervals from nonparametric bootstrap
are more accurate than $t$-intervals for larger sample sizes, they are
less accurate for smaller sample sizes. In the contect of structural equation
modeling, \citet{nevitt2001performance} find
that a sample size of 200--1000 is usually sufficient for interval estimation
using standard nonparametric bootstrap. For dependent data, the necessary sample
size for block bootstrap is even less clear. In a linear regression setting with
dependent data, \citet{goncalves2005bootstrap} find that, standard error
estimates from block bootstrap in small samples may be more accurate than
inference from closed-form asymptotic estimates, but block bootstrap percentile
confidence intervals still do not have sufficient coverage even for sample size
$n = 1024$.


The goal of this paper is to provide some practical recommendations on necessary
sample size for block bootstrap with dependent data. We consider a simple
situation of a stationary time series, where the parameters of interests are the
mean, standard deviation, and the first-order autocorrelation coefficient.
We compare four variants of confidence intervals from block bootstrap:
\jy{list them} literature \citep{diciccio1996bootstrap}
\citep{rice2006mathematical}.
Their empirical coverage rates at different sample sizes and dependence levels
are compared in a simulation study.
\jy{may reveal the most interesting result here to engage interests.}



The remainder of the papers is organized as follows.
\jy{refer to sections by label instead of hard number}
these intervals are constructed as well as the general procedure for block
bootstrap parameter estimation is reviewed in Section 2. Section 3 offers an
explanation of the simulation design and results. Section 4 discusses the main
takeaways from the results, how these findings could be useful, and possible
future explorations.


\section{Block Bootstrap Confidence Intervals}
\label{sec:blkbootreview}

\jy{set up notations for sample; give the algorithm to perform bootstrap
  procedure}

\jy{two subsections: 1) block bootstrap basics; 2) bootstrap CIs. The different
  CIs should each take a paragraph.}

\jy{Needs to distinguish the basic percentile bootstrap and corrected percentile
  bootstrap. Recall that we had trouble covering phi using the basic percentile
  method.}


Block bootstrap is a method that can be used to estimate a parameter of
serially dependent data. Suppose that one wants to estimate a parameter or
parameters $\theta$ of a population. The first step is to generate a sample of
length $n$ from the population. An estimate of the parameter
$\hat{\theta}_{n}$ can be computed from this sample. A pseudo-estimate
$\hat\theta_n^*$ can be computed from a bootstrapped pseudo-sample. The mean
$\bar\theta_n^*$ of many replicate $\hat\theta_n^*$ can also be computed.
Lastly, for each $\hat\theta_n^*$, $\delta^* = \hat\theta_n^* -
\bar\theta_n^*$ can be computed.


In basic bootstrap procedure, the new pseudo-sample of size $n$ would be
created by simply resampling observations from the original sample with
replacement. However, in the case of a time series, in order to account for
the temporal dependence, the time series can be split into blocks, typically
of the same size. The block should be of size $l$ large enough for each
pseudo-sample to exhibit some temporal dependence, yet small enough for there
to be some variability between each pseudo-sample. Ideally, as $n$ increases,
$l$ should also increase, but $l / n$ should decrease. To achieve this, $l$ is
often assigned a value as a function of $n$. A common function that is
considered the best by much previous literature is $l = \lceil n^{1/3} \rceil$
\citep{buhlmann1999block}. The designer of the study can choose whether the
blocks overlap or not. If the blocks do not overlap, it is called a non-moving
block bootstrap. In this case, the original sample is split evenly into
blocks, which will then be resampled to create new pseudo-samples. If the
blocks do overlap, it is called a moving block bootstrap. Blocks are resampled
randomly from the original sample, but they do not have to be from a set of
evenly spaced blocks. For moving block bootstrap, if $l \vert n$, a
bootstrapped pseudo-sample is created by taking a sample of $n / l$ blocks of
size $l$ with replacement to create a new pseudo-sample of size $n$. If
$l does not divide n$, the pseudo-sample is created by taking a sample of
$\lfloor n / l \rfloor$ blocks of size $l$ and one block of size
$(n \;\mathrm{mod})\; l$. An estimate $\hat\theta_n^*$ is computed from the
pseudo-sample. This procedure can be repeated many times to create a
distribution of $\hat\theta_n^*$. 


In this study, 1000 $\hat\theta_n^*$ were created for one simulation of block
bootstrap estimation. Using this distribution of $\hat\theta_n^*$, a
$1 - \alpha$/2 \% confidence interval for the parameter can be constructed. 
The most simplistic method for constructing this interval is the percentile
confidence interval. For a parameter $\theta$, the percentile $1 - \alpha$/2
\% confidence interval takes the form: 
\[ [\hat\theta_{n, \alpha/2}^*, \hat\theta_{n, 1 - \alpha/2}^*].\] 
Another method is the bias-corrected and accelerated 
(BCA) $1 - \alpha$/2 \% confidence interval, which takes the form: 
\[ [\hat\theta_{n, \alpha_1}^*,\hat\theta_{n, \alpha_2}^*].\] 
$\alpha_{1}$ and $\alpha_{2}$ are the cumulative probability of $z_{1}$ and
$z_{2}$, respectively, where:
\[z_{1} = \frac{z_{0} - z_{1 - \alpha/2}}{1 - a(z_{0} - z_{1 - \alpha/2})} +
z_{0}\] and
\[z_{2} = \frac{z_{0} + z_{1 - \alpha/2}}{1 - a(z_{0} + z_{1 - \alpha/2})} +
z_{0}.\] 
Where $z_0$ is the quantile function of the proportion of
$\hat\theta_n^* < \bar\theta_n^*$, and $a$ is the skewness of
$\hat{\theta}_{n, -i}$, where $\hat{\theta}_{n, -i}$ is the statistic of the
original sample computed without the $i^{th}$ block.
	
	
Both of these intervals work well when attempting to recover the mean and
standard deviation of a temporally dependent process, but their coverage of
the temporal dependence deteriorates as $n$ increases. A potential solution to
this issue is to center the intervals $\hat{\theta}_{n}$. To do this, the
distribution $\delta^*$ should be considered. A percentile interval centered
around $\hat{\theta}_{n}$ ($CI_{\hat{\theta}_{n}}$) takes the form:
\[ [\hat{\theta}_{n} + \delta^*_{\alpha/2},
  \hat{\theta}_{n} + \delta^*_{1 - \alpha/2}].\] 
This centering procedure can also be applied to a BCA interval. A BCA interval
centered around $\hat{\theta}_{n}$ ($CI_{BCA, \hat{\theta}_{n}}$) takes the
form:
\[ [\hat{\theta}_{n} + \delta^*_{\alpha_1},
  \hat{\theta}_{n} + \delta^*_{\alpha_2}].\] 


Thus, there are two types of intervals that can be constructed for block
bootstrap estimation in this study: $CI_{\hat{\theta}_{n}}$ and
$CI_{BCA, \hat{\theta}_{n}}$. Note that both of these intervals are centered
around $\hat{\theta}_{n}$ and asymmetric. 


\section{Simulation Study}
\label{sec:simstudy}


Base R has a built in function arima.sim that allows one to simulate an AR(1)
process, for which one can set the theoretical $\phi$. By default, the mean of
the process $\mu$ is 0 and the standard deviation of the error term
$\sigma_{\epsilon}$ is 1. For the purposes of this study, the standard
deviation of the process $\sigma_{x}$ should be controlled at 1. The
observations can be multiplied by $\sqrt{1 - \phi^2}$ so that the new
theoretical $\sigma_{x}$ is equivalent to 1, as shown below:
\begin{align}
X_{t} &= \epsilon_{t} + \phi X_{t-1};\\
Var(X_{t}) &= Var(\epsilon_{t} + \phi X_{t-1});\\
\sigma^2_{x} &= \sigma^2_{\epsilon} + \phi^2 \sigma^2_{x};\\
\sigma^2_{x}(1 - \phi^2) &= 1;\\
\sigma_{x}\sqrt{1 - \phi^2} &= 1.
\end{align}
The experimental factor of this study was $\phi$, as the temporal dependence
affects the performance of block bootstrap.
$\phi \in \{-0.4 -0.2, 0, 0.2, 0.4\}$.



As described in the last section, the block bootstrap method can be used to
estimate a parameter $\theta$ of an AR(1) process. This method can be
simulated with the tsboot function from the boot R package. In this study,
$\theta$ is composed of the target parameters $\mu = 0$, $\sigma_{x}$, and
$\phi$. The sample mean $\bar{x}$ can be used as the statistic to estimate
$\mu$, the sample standard deviation $S_x$ can be used as the statistic to
estimate $\sigma$, and the autocorrelation function of the sample at lag 1
$\rho_1$ can be used as the statistic to estimate $\phi$. In the R
implementation, these three statistics are incorporated into a function that
will apply each statistic to the original sample and to each bootstrapped
pseudo-sample. From a sample of length $n$, two types of block bootstrap 95\%
confidence intervals can be constructed for each target parameter:
$CI_{\hat{\theta}_{n}}$ and $CI_{BCA, \hat{\theta}_{n}}$.


Each of these 6 intervals either recovers $\theta$ or does not. Remember that
95\% of many of a certain type of 95\% confidence intervals constructed from
samples of the same $n$ should recover $\theta$. To see if this is
approximately the case, one can run 1000 replications of the block bootstrap
procedure, and for each interval-$\theta$ pair, the rate at which that type of
interval recovers $\theta$ can be recorded. The coverage rate is, of course, a
point estimate of a proportion, so a 95\% confidence interval of this coverage
proportion (with $n_{CI} = 1000$) can be constructed, and if the proportion
.95 is included in the interval, the block bootstrap method is likely working
well. If all values in the interval are below .95, the results suggest that the
method either is providing an inaccurate estimation, is underestimating the
process' variability, or a combination of both. If all values in the interval
are above .95, the results suggest that the method is overestimating the
process' variability.


If $\theta$ is not covered at that $n$, a larger $n$ may be
necessary, assuming that performance will improve as $n$ is increased. $n =$
100, 200, 300, 400, and 500, 600, 700 were used in this study. Remember that
the goal of this study is to find the smallest $n$ which is sufficient for
proper coverage. 


\begin{figure}[tbp]
  \centering
  \includegraphics[width=\textwidth]{figures/plot_n.4}
  \caption{At $\phi = -.4$, both intervals covered $\mu$ correctly as low as
    $n = 100$. $CI_{\hat{\theta}_{n}}$ seems to cover $\sigma$ well as low as
    $n = 400$. $CI_{BCA, \hat{\theta}_{n}}$ under-covers $\sigma$, but there
    is no clear pattern of deterioration of coverage, so one can expect that
    increasing $n$ will eventually result in proper coverage of $\sigma$.
    $CI_{\hat{\theta}_{n}}$ covers $\phi$ well with no deterioration, and this
    is true as low as $n = 200$. $CI_{BCA, \hat{\theta}_{n}}$ greatly
    under-covers $\phi$, and there is a clear pattern of deterioration of
    coverage.}
  \label{fig:plot_n.4}
\end{figure}


\begin{figure}[tbp]
  \centering
  \includegraphics[width=\textwidth]{figures/plot_n.2}
  \caption{At $\phi = -.2$, both intervals covered $\mu$ correctly as low as
    $n = 100$. Both intervals seem to cover $\sigma$ generally well, although
    $CI_{BCA, \hat{\theta}_{n}}$ has somewhat lower coverage rates.
    $CI_{\hat{\theta}_{n}}$ covers $\phi$ well with no deterioration of
    coverage, and this is true as low as $n = 200$.
    $CI_{BCA, \hat{\theta}_{n}}$ greatly under-covers $\phi$, and there is a
    clear coverage deterioration trend as $n$ increases.}
  \label{fig:plot_n.2}
\end{figure}


\begin{figure}[tbp]
  \centering
  \includegraphics[width=\textwidth]{figures/plot_0}
  \caption{At $\phi = 0$, both intervals covered $\mu$ correctly as low as
    $n = 500$. $CI_{\hat{\theta}_{n}}$ seems to cover $\sigma$ well as low as 
    $n = 400$. $CI_{BCA, \hat{\theta}_{n}}$ slightly under-covers $\sigma$, but
    there is no clear pattern of deterioration of coverage, so one can expect
    that increasing $n$ will eventually result in proper coverage of $\sigma$.
    $CI_{\hat{\theta}_{n}}$ seems to cover $\phi$ generally well at $n = 400$,
    but $CI_{BCA, \hat{\theta}_{n}}$ under-covers $\phi$, although the
    method's performance seems to improve as $n$ increases.}
  \label{fig:plot_0}
\end{figure}


\begin{figure}[tbp]
  \centering
  \includegraphics[width=\textwidth]{figures/plot_.2}
  \caption{At $\phi = .2$, both intervals covered $\mu$ correctly as low as
    $n = 300$. Both intervals seem to cover $\sigma$ generally well, although
    $CI_{BCA, \hat{\theta}_{n}}$ has somewhat lower coverage rates.
    $CI_{\hat{\theta}_{n}}$ covers $\phi$ well with no deterioration of
    coverage, and this is true as low as $n = 200$.
    $CI_{BCA, \hat{\theta}_{n}}$ greatly under-covers $\phi$, but there is no
    clear coverage deterioration trend as $n$ increases.}
  \label{fig:plot_.2}
\end{figure}


\begin{figure}[tbp]
  \centering
  \includegraphics[width=\textwidth]{figures/plot_.4}
  \caption{At $\phi = .4$, both intervals seem to cover $\mu$ generally well
    as low as $n = 400$. $CI_{\hat{\theta}_{n}}$ covers $\sigma$ well as low
    as $n = 400$. $CI_{BCA, \hat{\theta}_{n}}$ under-covers $\sigma$, but
    there is no clear sign of coverage deterioration, so one can expect that
    increasing $n$ will eventually result in proper coverage of $\sigma$.
    $CI_{\hat{\theta}_{n}}$ covers $\phi$ well with no deterioration of
    coverage, and this is true as low as $n = 200$.
    $CI_{BCA, \hat{\theta}_{n}}$ greatly under-covers $\phi$ with a clear
    coverage deterioration trend as $n$ increases.}
  \label{fig:plot_.4}
\end{figure}


As shown in the above figures, only $CI_{\hat{\theta}_{n}}$ managed to solve
the deterioration problem with respect to recovering $\phi$, indicating that
this interval should be used as opposed to the more complex
$CI_{BCA, \hat{\theta}_{n}}$ when estimating $\phi$. Increasing $n$ will not
fix the performance of $CI_{BCA, \hat{\theta}_{n}}$ because the assumption that
the coverage rate will approach an upper limit of .95 as $n$ increases is
invalid. As expected for $CI_{\hat{\theta}_{n}}$, a higher $\phi$ demands a
larger $n$ to estimate all parameters, but a negative $\phi$ may require a
lesser $n$. For example, for both $\phi = 0.2$ and $0.4$, $n = 100$ was
sufficient to recover $\mu$ at a rate of approximately 95\%.


\section{Concluding Remarks}
\label{sec:conremarks}

The motivation for the study is the idea that block bootstrap procedure will
perfectly estimate a parameter of a time series given an infinitely large
sample. The goal for this study was to find what is a good enough finite
sample length for block bootstrap estimation to recover the parameter of a
time series at an acceptable rate. Again, this relies on the assumption that
there is a size $n$ large enough for the method to work: that is, the method's
performance improves as $n$ increases. Out of the two types of intervals used
in this study, this assumption was found to hold true with respect to
estimating $\phi$ only for $CI_{\hat{\theta}_{n}}$, whereas
$CI_{BCA, \hat{\theta}_{n}}$ exhibited coverage deterioration as $n$
increased. When using $CI_{\hat{\theta}_{n}}$ and $\phi$ is unknown, the
results of this study suggest that an $n$ of around 500 may be necessary to
estimate $\mu$, and an $n$ of around 400 may be necessary to estimate
$\sigma$. If $\phi$ is already known, a lesser $n$ may be adequate to estimate
these parameters. When $\phi$ was negative, a $n$ of around 100 was sufficient
to estimate $\mu$. However, in real world applications, $\phi$ is more
commonly found to be positive, so a larger $n$ is more likely to be necessary.
Lastly, to estimate $\phi$, an $n$ of around 400 may be necessary. This
information could prove to be useful for researching using block bootstrap
estimation of time series in domains such as econometrics. Future studies
could investigate if there are types of block bootstrap intervals
constructions that would more appropriately recover the parameters of a time
series. One could also investigate the $n$ needed to make inferences about
other forms of serially dependent data such as an MA process.


\bibliographystyle{chicago}
\bibliography{citations}[tp]

\end{document}

